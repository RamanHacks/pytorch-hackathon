{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER for Document Extraction Tool\n",
    "This notebook contains script to train NER and run inference using the LayoutLM model \"[LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318)\" by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei and Ming Zhou as implemented in Huggingface Transformers library. We take open-source receipt data from SROIE competition and form data from FUNSD repo. For more details, visit these links:\n",
    "- LayoutLM Github repo [here](https://github.com/microsoft/unilm/tree/master/layoutlm).\n",
    "- Read about the SROIE competition and dataset [here](https://rrc.cvc.uab.es/?ch=13).\n",
    "- More about FUNSD [here] (https://guillaumejaume.github.io/FUNSD/).\n",
    "- \"Fine tune SROIE on LayoutLM\" by ruifcruz [here](https://github.com/ruifcruz/sroie-on-layoutlm).\n",
    "- Notebook is inspired from Neils Rogge transformer's [tutorials](https://github.com/NielsRogge/Transformers-Tutorials) and Urban Knuples kaggle [notebooks](https://www.kaggle.com/urbikn/layoutlm-using-the-sroie-dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing SROIE dataset\n",
    "Before fine-tuning the model, we have to preprocess the SROIE dataset which can be downloaded from [here](https://drive.google.com/drive/folders/1ShItNWXyiY1tFDM5W02bceHuJjyeeJl2). The dataset contains multiple subfolders, because the competition is split up into three tasks **Text Localization, Optical character recognition (OCR)** and **Information Extraction (IE)** and some folders are meant for their specific task. For our purposes we're only interested in the last task, so we'll be using these two folders: \n",
    "- **0325updated.task1train(626p)** - contains receipt images (.jpg) and corresponding OCR'd bounding boxes and text (.txt)\n",
    "- **0325updated.task2train(626p)** - contains labeled text (.txt) in a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib\n",
    "from matplotlib import pyplot, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset\n",
    "The location of the SROIE dataset and the name of an example file used for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sroie_folder_path = Path('/home/fsmlp/Downloads/SROIE2019')\n",
    "example_file = Path('X51005757324.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(points: list, width: int, height: int) -> list:\n",
    "  x0, y0, x2, y2 = [int(p) for p in points]\n",
    "  \n",
    "  x0 = int(1000 * (x0 / width))\n",
    "  x2 = int(1000 * (x2 / width))\n",
    "  y0 = int(1000 * (y0 / height))\n",
    "  y2 = int(1000 * (y2 / height))\n",
    "\n",
    "  return [x0, y0, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = '/home/fsmlp/Downloads/SROIE2019/test_data/'\n",
    "name='test'\n",
    "# input_folder = '/home/fsmlp/Downloads/SROIE2019/0325updated.task2train(626p)/'\n",
    "# name='train'\n",
    "\n",
    "output_dir = '/home/fsmlp/Downloads/SROIE2019/datan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = input_folder[:-1] + '-receiptResults.csv'\n",
    "df_csv = pd.read_csv(csv, delimiter='\\t')\n",
    "rejected_list = []\n",
    "for i in range(len(df_csv)):\n",
    "    f = input_folder + df_csv['Filename'][i][:-3] + 'json'\n",
    "    with open(f,'r') as fil:\n",
    "        data = fil.read()\n",
    "    js = json.loads(data) \n",
    "    width,height = js['analyzeResult']['readResults'][0]['width'], js['analyzeResult']['readResults'][0]['height']\n",
    "    # rel_box = [w,h,w,h,w,h,w,h]\n",
    "\n",
    "    vals = df_csv['MerchantAddress-elements'][i]\n",
    "    bio_list = []\n",
    "    if not (isinstance(vals,float) and math.isnan(vals)):\n",
    "        vals = eval(vals)\n",
    "        first = True\n",
    "        for v in vals:\n",
    "            bio={}\n",
    "            i1 = v.split('/')[-3]\n",
    "            i2 = v.split('/')[-1]\n",
    "            # print(i1, i2)\n",
    "            tok_det = js['analyzeResult']['readResults'][0]['lines'][int(i1)]['words'][int(i2)]\n",
    "            bbox = tok_det['boundingBox']\n",
    "            bio['x0'] = min(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "            bio['y0'] = min(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "            bio['x2'] = max(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "            bio['y2'] = max(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "            bio['line'] = tok_det[\"text\"]\n",
    "            if bio['x2'] < bio['x0'] or bio['y2'] < bio['y0']:\n",
    "                print(\"Emptyyyyyyyyyyyyy\")\n",
    "            if first:\n",
    "                bio['label'] = \"B-Address\"\n",
    "                first=False\n",
    "            else:\n",
    "                bio['label'] = \"I-Address\" \n",
    "            bio['val'] = int(i1+i2)\n",
    "            bio_list.append(bio)\n",
    "    else:\n",
    "        # print(df_csv['Filename'][i])\n",
    "        rejected_list.append(df_csv['Filename'][i])\n",
    "        continue\n",
    "        # vals = eval(df_csv[t+'-elements'][i])\n",
    "    \n",
    "    vals = df_csv['Total-elements'][i]\n",
    "    tot = True\n",
    "    if (isinstance(vals,float) and math.isnan(vals)):\n",
    "        new_vals = df_csv['Subtotal-elements'][i]\n",
    "        if not (isinstance(vals,float) and math.isnan(vals)):\n",
    "            vals=new_vals\n",
    "        else:\n",
    "            tot=False \n",
    "    if tot:\n",
    "        vals = eval(vals)\n",
    "        first = True\n",
    "        for v in vals:\n",
    "            bio={}\n",
    "            i1 = v.split('/')[-3]\n",
    "            i2 = v.split('/')[-1]\n",
    "            # print(i1, i2)\n",
    "            tok_det = js['analyzeResult']['readResults'][0]['lines'][int(i1)]['words'][int(i2)]\n",
    "            bbox = tok_det['boundingBox']\n",
    "            bio['x0'] = min(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "            bio['y0'] = min(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "            bio['x2'] = max(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "            bio['y2'] = max(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "            bio['line'] = tok_det[\"text\"]\n",
    "            if bio['x2'] < bio['x0'] or bio['y2'] < bio['y0']:\n",
    "                print(\"Emptyyyyyyyyyyyyy\")\n",
    "            if first:\n",
    "                bio['label'] = \"B-Total\"\n",
    "                first=False\n",
    "            else:\n",
    "                bio['label'] = \"I-Total\" \n",
    "            bio['val'] = int(i1+i2)\n",
    "            bio_list.append(bio)\n",
    "    \n",
    "    vals = df_csv['MerchantName-elements'][i]\n",
    "    if not (isinstance(vals,float) and math.isnan(vals)):\n",
    "        vals = eval(vals)\n",
    "        first = True\n",
    "        for v in vals:\n",
    "            bio={}\n",
    "            i1 = v.split('/')[-3]\n",
    "            i2 = v.split('/')[-1]\n",
    "            # print(i1, i2)\n",
    "            tok_det = js['analyzeResult']['readResults'][0]['lines'][int(i1)]['words'][int(i2)]\n",
    "            bbox = tok_det['boundingBox']\n",
    "            bio['x0'] = min(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "            bio['y0'] = min(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "            bio['x2'] = max(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "            bio['y2'] = max(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "            bio['line'] = tok_det[\"text\"]\n",
    "            if bio['x2'] < bio['x0'] or bio['y2'] < bio['y0']:\n",
    "                print(\"Emptyyyyyyyyyyyyy\")\n",
    "            if first:\n",
    "                bio['label'] = \"B-Company\"\n",
    "                first=False\n",
    "            else:\n",
    "                bio['label'] = \"I-Company\" \n",
    "            bio['val'] = int(i1+i2)\n",
    "            bio_list.append(bio)\n",
    "            \n",
    "    vals = df_csv['TransactionDate-elements'][i]\n",
    "    if not (isinstance(vals,float) and math.isnan(vals)):\n",
    "        vals = eval(vals)\n",
    "        first = True\n",
    "        for v in vals:\n",
    "            bio={}\n",
    "            i1 = v.split('/')[-3]\n",
    "            i2 = v.split('/')[-1]\n",
    "            # print(i1, i2)\n",
    "            tok_det = js['analyzeResult']['readResults'][0]['lines'][int(i1)]['words'][int(i2)]\n",
    "            bbox = tok_det['boundingBox']\n",
    "            bio['x0'] = min(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "            bio['y0'] = min(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "            bio['x2'] = max(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "            bio['y2'] = max(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "            bio['line'] = tok_det[\"text\"]\n",
    "            if bio['x2'] < bio['x0'] or bio['y2'] < bio['y0']:\n",
    "                print(\"Emptyyyyyyyyyyyyy\")\n",
    "            if first:\n",
    "                bio['label'] = \"B-Date\"\n",
    "                first=False\n",
    "            else:\n",
    "                bio['label'] = \"I-Date\" \n",
    "            bio['val'] = int(i1+i2)\n",
    "            bio_list.append(bio)\n",
    "            \n",
    "    df_temp = pd.DataFrame(bio_list)\n",
    "    # print(df_temp)\n",
    "    final_list = []\n",
    "    for y,te in enumerate(js[\"analyzeResult\"]['readResults'][0][\"lines\"]):\n",
    "        for z,wo in enumerate(te['words']):\n",
    "            search=int(str(y)+str(z))\n",
    "            biol = df_temp['val'].tolist()\n",
    "            final = {}\n",
    "            try:\n",
    "                a = biol.index(search)\n",
    "                final['x0'] = df_temp['x0'][a]\n",
    "                final['y0'] = df_temp['y0'][a]\n",
    "                final['x2'] = df_temp['x2'][a]\n",
    "                final['y2'] = df_temp['y2'][a]\n",
    "                final['line'] = wo['text']\n",
    "                final['label'] = df_temp['label'][a]\n",
    "            except ValueError:\n",
    "                ct=0\n",
    "                bbox = wo['boundingBox']\n",
    "                final['x0'] = min(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "                final['y0'] = min(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "                final['x2'] = max(int(bbox[0]),int(bbox[2]),int(bbox[4]),int(bbox[6]))\n",
    "                final['y2'] = max(int(bbox[1]),int(bbox[3]),int(bbox[5]),int(bbox[7]))\n",
    "                final['line'] = wo['text']\n",
    "                if final['x2'] < final['x0'] or final['y2'] < final['y0']:\n",
    "                    # print(\"Issue Hereeeeeeeeeeeeeeeeeeeeeeeeeee\")\n",
    "                    print(df_csv['Filename'][i],final[\"line\"])\n",
    "                    print()\n",
    "                    # break\n",
    "                final['label'] = 'O'\n",
    "            final_list.append(final)\n",
    "    tm = pd.DataFrame(final_list)[['x0','y0','x2','y2','line','label']]\n",
    "    tm.to_csv(input_folder + df_csv['Filename'][i][:-3]+'csv',sep='\\t', index=False)\n",
    "    \n",
    "    with open(f\"{output_dir}/{name}.txt\", \"a\", encoding=\"utf8\") as file, \\\n",
    "         open(f\"{output_dir}/{name}_box.txt\", \"a\", encoding=\"utf8\") as file_bbox, \\\n",
    "         open(f\"{output_dir}/{name}_image.txt\", \"a\", encoding=\"utf8\") as file_image:\n",
    "        for index, row in tm.iterrows():\n",
    "            bbox = [int(p) for p in row[['x0', 'y0', 'x2', 'y2']]]\n",
    "            normalized_bbox = normalize(bbox, width, height)\n",
    "\n",
    "            file.write(\"{}\\t{}\\n\".format(row['line'], row['label']))\n",
    "            file_bbox.write(\"{}\\t{} {} {} {}\\n\".format(row['line'], *normalized_bbox))\n",
    "            file_image.write(\"{}\\t{} {} {} {}\\t{} {}\\t{}\\n\".format(row['line'], *bbox, width, height, df_csv['Filename'][i][:-4]))\n",
    "\n",
    "        # Write a second newline to separate dataset from others\n",
    "        file.write(\"\\n\")\n",
    "        file_bbox.write(\"\\n\")\n",
    "        file_image.write(\"\\n\")\n",
    "    # break        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the words and bounding boxes\n",
    "So, the first step is reading the OCR data, where every line in the file includes a group of words and a bounding box which defines them. All we have to do is read the file, discard the unneeded points in the bounding box (because the model requires only the top-left and bottom-right points) and save it in Pandas Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the entities file\n",
    "Now we need to read the entities file to know what to label in our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== File content ==\n",
      "{\n",
      "    \"company\": \"MR. D.I.Y. (M) SDN BHD\",\n",
      "    \"date\": \"25-03-18\",\n",
      "    \"address\": \"LOT 1851-A & 1851-B, JALAN KPB 6, KAWASAN PERINDUSTRIAN BALAKONG, 43300 SERI KEMBANGAN, SELANGOR\",\n",
      "    \"total\": \"50.80\"\n",
      "}\n",
      "\n",
      "== Dataframe ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>address</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MR. D.I.Y. (M) SDN BHD</td>\n",
       "      <td>25-03-18</td>\n",
       "      <td>LOT 1851-A &amp; 1851-B, JALAN KPB 6, KAWASAN PERI...</td>\n",
       "      <td>50.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  company      date  \\\n",
       "0  MR. D.I.Y. (M) SDN BHD  25-03-18   \n",
       "\n",
       "                                             address  total  \n",
       "0  LOT 1851-A & 1851-B, JALAN KPB 6, KAWASAN PERI...  50.80  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_entities(path: Path):\n",
    "  with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "  dataframe = pd.DataFrame([data])\n",
    "  return dataframe\n",
    "\n",
    "\n",
    "# Example usage\n",
    "entities_file_path = sroie_folder_path / \"0325updated.task2train(626p)\" / example_file\n",
    "print(\"== File content ==\")\n",
    "!head \"{entities_file_path}\"\n",
    "\n",
    "entities = read_entities(path=entities_file_path)\n",
    "print(\"\\n\\n== Dataframe ==\")\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning labels to words using the entities data\n",
    "We have our words/lines and entities, now we just need to put them together by labeling our lines using the entities values. We'll be doing that by substring matching the entities values with the lines and if they don't match to a similarity check using pythons _difflib.SequenceMatcher_ and assigning anything above the 0.8 (80%) prediction match.\n",
    "\n",
    "The **label \"O\"** will define all our words not labeled during the assignment step, because it's required for us to label everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign a label to the line by checking the similarity\n",
    "# of the line and all the entities\n",
    "def assign_line_label(line: str, entities: pd.DataFrame):\n",
    "    line_set = line.replace(\",\", \"\").strip().split()\n",
    "    for i, column in enumerate(entities):\n",
    "        entity_values = entities.iloc[0, i].replace(\",\", \"\").strip()\n",
    "        entity_set = entity_values.split()\n",
    "\n",
    "        matches_count = 0\n",
    "        for l in line_set:\n",
    "          if any(SequenceMatcher(a=l, b=b).ratio() > 0.8 for b in entity_set):\n",
    "            matches_count += 1\n",
    "\n",
    "        if matches_count == len(line_set) or matches_count == len(entity_set):\n",
    "            return column.upper()\n",
    "\n",
    "    return \"O\"\n",
    "\n",
    "\n",
    "line = bbox.loc[0,\"line\"]\n",
    "label = assign_line_label(line, entities)\n",
    "print(\"Line:\", line)\n",
    "print(\"Assigned label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a function which can handle the labeling of our lines, we'll create another function to label all our line in one DataFrame (so one receipt).\n",
    "\n",
    "As simple as this could be, the problem arises when we get lines which would all pass the same match, like **TOTAL** for example; a receipt could have only one item on it and its price could be the same as the final total, so duplicate labels. Or maybe part of the address is also present at the end of the receipt.\n",
    "\n",
    "To ignore such examples, I wrote simple hard-coded rules to assign *total* and *date* to only the largest bounding boxes it could find (based on its area) and to not allow the address to be assigned after date or total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign_labels(words: pd.DataFrame, entities: pd.DataFrame):\n",
    "    max_area = {\"TOTAL\": (0, -1), \"DATE\": (0, -1)}  # Value, index\n",
    "    already_labeled = {\"TOTAL\": False,\n",
    "                       \"DATE\": False,\n",
    "                       \"ADDRESS\": False,\n",
    "                       \"COMPANY\": False,\n",
    "                       \"O\": False\n",
    "    }\n",
    "\n",
    "    # Go through every line in $words and assign it a label\n",
    "    labels = []\n",
    "    for i, line in enumerate(words['line']):\n",
    "        label = assign_line_label(line, entities)\n",
    "\n",
    "        already_labeled[label] = True\n",
    "        if label == \"ADDRESS\" and (already_labeled[\"DATE\"] or already_labeled[\"TOTAL\"]):\n",
    "            label = \"O\"\n",
    "\n",
    "        # Assign to the largest bounding box\n",
    "        if label in [\"TOTAL\", \"DATE\"]:\n",
    "            x0_loc = words.columns.get_loc(\"x0\")\n",
    "            bbox = words.iloc[i, x0_loc:x0_loc+4].to_list()\n",
    "            area = (bbox[2] - bbox[0]) + (bbox[3] - bbox[1])\n",
    "\n",
    "            if max_area[label][0] < area:\n",
    "                max_area[label] = (area, i)\n",
    "\n",
    "            label = \"O\"\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    labels[max_area[\"DATE\"][1]] = \"DATE\"\n",
    "    labels[max_area[\"TOTAL\"][1]] = \"TOTAL\"\n",
    "\n",
    "    words[\"label\"] = labels\n",
    "    return words\n",
    "\n",
    "\n",
    "# Example usage\n",
    "bbox_labeled = assign_labels(bbox, entities)\n",
    "bbox_labeled.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split words\n",
    "For the last part we're splitting the lines into separate tokens with their own bounding boxes.\n",
    "\n",
    "Splitting the bounding boxes based on word length is probably not the best approach, but it's good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_line(line: pd.Series):\n",
    "  line_copy = line.copy()\n",
    "\n",
    "  line_str = line_copy.loc[\"line\"]\n",
    "  words = line_str.split(\" \")\n",
    "\n",
    "  # Filter unwanted tokens\n",
    "  words = [word for word in words if len(word) >= 1]\n",
    "\n",
    "  x0, y0, x2, y2 = line_copy.loc[['x0', 'y0', 'x2', 'y2']]\n",
    "  bbox_width = x2 - x0\n",
    "  \n",
    "\n",
    "  new_lines = []\n",
    "  for index, word in enumerate(words):\n",
    "    x2 = x0 + int(bbox_width * len(word)/len(line_str))\n",
    "    line_copy.at['x0', 'x2', 'line'] = [x0, x2, word]\n",
    "    new_lines.append(line_copy.to_list())\n",
    "    x0 = x2 + 5 \n",
    "\n",
    "  return new_lines\n",
    "\n",
    "\n",
    "# Example usage\n",
    "new_lines = split_line(bbox_labeled.loc[0])\n",
    "print(\"Original row:\")\n",
    "display(bbox_labeled.loc[0:0,:])\n",
    "\n",
    "print(\"Splitted row:\")\n",
    "pd.DataFrame(new_lines, columns=bbox_labeled.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "We defined all our functions, now we just have to use them on every file and transform the dataset into a format which the script/model can parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "def dataset_creator(folder: Path, total=1000):\n",
    "  bbox_folder = folder / '0325updated.task1train(626p)'\n",
    "  entities_folder = folder / '0325updated.task2train(626p)'\n",
    "\n",
    "  # Ignoring unwanted files which produced problems when I wanted to fine-tune the model with them included\n",
    "  ignore = ['X51006619545.txt', 'X51006619785.txt', 'X51005663280(1).txt', 'X51005663280.txt'] \n",
    "  files = [file for file in bbox_folder.glob(\"*.txt\") if file.name not in ignore]\n",
    "  files = files[:total]\n",
    "\n",
    "  data = []\n",
    "\n",
    "  print(\"Reading dataset:\")\n",
    "  for file in tqdm(files, total=len(files)):\n",
    "    bbox_file_path = file\n",
    "    entities_file_path = entities_folder / file.name\n",
    "    image_file_path = bbox_folder / file.with_suffix(\".jpg\")\n",
    "  \n",
    "    # Check if all the required files exist\n",
    "    if not bbox_file_path.is_file() or not entities_file_path.is_file() or not image_file_path.is_file():\n",
    "      continue\n",
    "  \n",
    "    # Read the files\n",
    "    bbox = read_bbox_and_words(bbox_file_path)\n",
    "    entities = read_entities(entities_file_path)\n",
    "    image = Image.open(image_file_path)\n",
    "\n",
    "    # Assign labels to lines in bbox using entities\n",
    "    bbox_labeled = assign_labels(bbox, entities)\n",
    "    del bbox\n",
    "\n",
    "    # Split lines into separate tokens\n",
    "    new_bbox_l = []\n",
    "    for index, row in bbox_labeled.iterrows():\n",
    "      new_bbox_l += split_line(row)\n",
    "    new_bbox = pd.DataFrame(new_bbox_l, columns=bbox_labeled.columns, dtype=np.int16)\n",
    "    del bbox_labeled\n",
    "\n",
    "\n",
    "    # Do another label assignment to keep the labeling more precise \n",
    "    for index, row in new_bbox.iterrows():\n",
    "      label = row['label']\n",
    "\n",
    "      if label != \"O\":\n",
    "        entity = entities.iloc[0, entities.columns.get_loc(label.lower())]\n",
    "        if row['line'] not in entity:\n",
    "          label = \"O\"\n",
    "        else:\n",
    "            # Not really IOB tagging, but it gives the best results\n",
    "            label = \"S-\" + label\n",
    "      \n",
    "      new_bbox.at[index, 'label'] = label\n",
    "\n",
    "    width, height = image.size\n",
    "  \n",
    "    data.append([new_bbox, width, height])\n",
    "  return data\n",
    "\n",
    "dataset = dataset_creator(sroie_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the dataset into training and testing files\n",
    "With our dataset transformed, we'll split the dataset into a trainable and testable set. I'm allocating 80% of the dataset to training and the other 20% to testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.Random(4).shuffle(dataset)\n",
    "\n",
    "# Set split point to be 80% of the dataset\n",
    "split_point = int(len(dataset) * 0.8) \n",
    "\n",
    "dataset_train  = dataset[:split_point]\n",
    "dataset_test = dataset[split_point:]\n",
    "del(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the writing function\n",
    "We'll use the same function to write into the train and test files\n",
    "\n",
    "The normalization function is meant to normalize the bounding boxes points in a range [0,1000] using the width and height of the image of the receipt [\\[source\\]](https://huggingface.co/transformers/model_doc/layoutlm.html#overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(points: list, width: int, height: int) -> list:\n",
    "  x0, y0, x2, y2 = [int(p) for p in points]\n",
    "  \n",
    "  x0 = int(1000 * (x0 / width))\n",
    "  x2 = int(1000 * (x2 / width))\n",
    "  y0 = int(1000 * (y0 / height))\n",
    "  y2 = int(1000 * (y2 / height))\n",
    "\n",
    "  return [x0, y0, x2, y2]\n",
    "\n",
    "\n",
    "def write_dataset(dataset: list, output_dir: Path, name: str):\n",
    "  print(f\"Writing {name}ing dataset:\")\n",
    "  with open(output_dir / f\"{name}.txt\", \"w+\", encoding=\"utf8\") as file, \\\n",
    "       open(output_dir / f\"{name}_box.txt\", \"w+\", encoding=\"utf8\") as file_bbox, \\\n",
    "       open(output_dir / f\"{name}_image.txt\", \"w+\", encoding=\"utf8\") as file_image:\n",
    "\n",
    "      # Go through each dataset\n",
    "      for datas in tqdm(dataset, total=len(dataset)):\n",
    "        data, width, height = datas\n",
    "        \n",
    "        filename = data.iloc[0, data.columns.get_loc('filename')]\n",
    "\n",
    "        # Go through every row in dataset\n",
    "        for index, row in data.iterrows():\n",
    "          bbox = [int(p) for p in row[['x0', 'y0', 'x2', 'y2']]]\n",
    "          normalized_bbox = normalize(bbox, width, height)\n",
    "\n",
    "          file.write(\"{}\\t{}\\n\".format(row['line'], row['label']))\n",
    "          file_bbox.write(\"{}\\t{} {} {} {}\\n\".format(row['line'], *normalized_bbox))\n",
    "          file_image.write(\"{}\\t{} {} {} {}\\t{} {}\\t{}\\n\".format(row['line'], *bbox, width, height, filename))\n",
    "\n",
    "        # Write a second newline to separate dataset from others\n",
    "        file.write(\"\\n\")\n",
    "        file_bbox.write(\"\\n\")\n",
    "        file_image.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_directory = Path('/home/fsmlp/Downloads/SROIE2019','data')\n",
    "\n",
    "dataset_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "write_dataset(dataset_train, dataset_directory, 'train')\n",
    "write_dataset(dataset_test, dataset_directory, 'test')\n",
    "\n",
    "# Creating the 'labels.txt' file to the the model what categories to predict.\n",
    "labels = ['COMPANY', 'DATE', 'ADDRESS', 'TOTAL']\n",
    "IOB_tags = ['S']\n",
    "with open(dataset_directory / 'labels.txt', 'w') as f:\n",
    "  for tag in IOB_tags:\n",
    "    for label in labels:\n",
    "      f.write(f\"{tag}-{label}\\n\")\n",
    "  f.write(\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Fine tune LayoutLM\n",
    "We downloaded and transformed our dataset into a trainable and testable set, now we can start the fine-tuning of the model.\n",
    "\n",
    "## Download the model\n",
    "First we're going to clone the LayoutLM Github project which contains the script to fine tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'unilm'...\n",
      "remote: Enumerating objects: 3210, done.\u001b[K\n",
      "remote: Counting objects: 100% (2344/2344), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1813/1813), done.\u001b[K\n",
      "remote: Total 3210 (delta 1000), reused 1647 (delta 455), pack-reused 866\u001b[K\n",
      "Receiving objects: 100% (3210/3210), 4.99 MiB | 6.88 MiB/s, done.\n",
      "Resolving deltas: 100% (1480/1480), done.\n",
      "Processing ./unilm/layoutlm\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: transformers==2.9.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from layoutlm==0.0) (2.9.0)\n",
      "Requirement already satisfied: tensorboardX==2.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from layoutlm==0.0) (2.0)\n",
      "Requirement already satisfied: lxml==4.5.1 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from layoutlm==0.0) (4.5.1)\n",
      "Requirement already satisfied: seqeval==0.0.12 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from layoutlm==0.0) (0.0.12)\n",
      "Requirement already satisfied: Pillow==7.1.2 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from layoutlm==0.0) (7.1.2)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from seqeval==0.0.12->layoutlm==0.0) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from seqeval==0.0.12->layoutlm==0.0) (1.21.4)\n",
      "Requirement already satisfied: six in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from tensorboardX==2.0->layoutlm==0.0) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from tensorboardX==2.0->layoutlm==0.0) (3.19.1)\n",
      "Requirement already satisfied: filelock in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==2.9.0->layoutlm==0.0) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==2.9.0->layoutlm==0.0) (2021.10.23)\n",
      "Requirement already satisfied: sacremoses in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==2.9.0->layoutlm==0.0) (0.0.46)\n",
      "Requirement already satisfied: requests in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==2.9.0->layoutlm==0.0) (2.26.0)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==2.9.0->layoutlm==0.0) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==2.9.0->layoutlm==0.0) (4.62.3)\n",
      "Requirement already satisfied: sentencepiece in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==2.9.0->layoutlm==0.0) (0.1.96)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers==2.9.0->layoutlm==0.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers==2.9.0->layoutlm==0.0) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers==2.9.0->layoutlm==0.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers==2.9.0->layoutlm==0.0) (2.0.4)\n",
      "Requirement already satisfied: joblib in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from sacremoses->transformers==2.9.0->layoutlm==0.0) (1.1.0)\n",
      "Requirement already satisfied: click in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from sacremoses->transformers==2.9.0->layoutlm==0.0) (8.0.3)\n",
      "Building wheels for collected packages: layoutlm\n",
      "  Building wheel for layoutlm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for layoutlm: filename=layoutlm-0.0-py3-none-any.whl size=11486 sha256=fb301d2bae0294871765fd16a9a5e3a56f623ef017c306097803abb8fe5c6df0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d4ua_hmd/wheels/03/3e/4b/d439001055c0efe35ddaa69327f435b032a6dfa8cbfbfbcd42\n",
      "Successfully built layoutlm\n",
      "Installing collected packages: layoutlm\n",
      "  Attempting uninstall: layoutlm\n",
      "    Found existing installation: layoutlm 0.0\n",
      "    Uninstalling layoutlm-0.0:\n",
      "      Successfully uninstalled layoutlm-0.0\n",
      "Successfully installed layoutlm-0.0\n"
     ]
    }
   ],
   "source": [
    "# ! rm -r unilm\n",
    "! git clone -b remove_torch_save https://github.com/NielsRogge/unilm.git\n",
    "! cd unilm/layoutlm\n",
    "! pip install unilm/layoutlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 88571, done.\u001b[K\n",
      "remote: Counting objects: 100% (282/282), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 88571 (delta 281), reused 279 (delta 279), pack-reused 88289\u001b[K\n",
      "Receiving objects: 100% (88571/88571), 71.29 MiB | 14.76 MiB/s, done.\n",
      "Resolving deltas: 100% (63821/63821), done.\n",
      "Processing ./transformers\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (1.21.4)\n",
      "Requirement already satisfied: requests in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (2.26.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: sacremoses in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (0.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (21.0)\n",
      "Requirement already satisfied: filelock in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from transformers==4.13.0.dev0) (2021.10.23)\n",
      "Requirement already satisfied: typing-extensions in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers==4.13.0.dev0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers==4.13.0.dev0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers==4.13.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from requests->transformers==4.13.0.dev0) (3.2)\n",
      "Requirement already satisfied: click in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from sacremoses->transformers==4.13.0.dev0) (8.0.3)\n",
      "Requirement already satisfied: six in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from sacremoses->transformers==4.13.0.dev0) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/fsmlp/miniconda3/envs/torch-gpu/lib/python3.8/site-packages (from sacremoses->transformers==4.13.0.dev0) (1.1.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3101540 sha256=424291207c488346f2de35fd7fb23a079a958ad10f8fdf2fa9d5e96dfbe8f366\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6cjcsomc/wheels/f0/1f/52/e70d97d0075eea6e34d99195e2c5920e8c99fe9c8ecb09ae67\n",
      "Successfully built transformers\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.7.0\n",
      "    Uninstalling tokenizers-0.7.0:\n",
      "      Successfully uninstalled tokenizers-0.7.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 2.9.0\n",
      "    Uninstalling transformers-2.9.0:\n",
      "      Successfully uninstalled transformers-2.9.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "layoutlm 0.0 requires transformers==2.9.0, but you have transformers 4.13.0.dev0 which is incompatible.\u001b[0m\n",
      "Successfully installed tokenizers-0.10.3 transformers-4.13.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# ! rm -r transformers\n",
    "! git clone https://github.com/huggingface/transformers.git\n",
    "! cd transformers\n",
    "! pip install ./transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC9FhkG9U8yg"
   },
   "source": [
    "## Define a PyTorch dataset\n",
    "\n",
    "First, we create a list containing the unique labels based on `data/labels.txt` (run this from the content directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://t3638486.p.clickup-attachments.com/t3638486/1678c4f6-0f92-484b-9e3f-0ea8275ad7cc/datan.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/home/fsmlp/Downloads/SROIE2019/datan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "675rRa0QXnMp"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def get_labels(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "    if \"O\" not in labels:\n",
    "        labels = [\"O\"] + labels\n",
    "    return labels\n",
    "\n",
    "labels = get_labels(output_dir+\"/labels.txt\")\n",
    "num_labels = len(labels)\n",
    "label_map = {i: label for i, label in enumerate(labels)}\n",
    "# Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "# pad_token_label_id = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-qXLkP9Yq_L",
    "outputId": "69f18363-bf64-4658-9f53-e491c587b8f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Company', 'I-Company', 'B-Address', 'I-Address', 'B-Total', 'I-Total', 'B-Date', 'I-Date', 'O'] -100\n"
     ]
    }
   ],
   "source": [
    "print(labels, pad_token_label_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_ck0ZFfZInR"
   },
   "source": [
    "Next, we can create a PyTorch dataset and corresponding dataloader (both for training and evaluation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HUJftzeBWh2S"
   },
   "outputs": [],
   "source": [
    "from transformers import LayoutLMTokenizer\n",
    "from layoutlm.data.funsd import FunsdDataset, InputFeatures\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "args = {'local_rank': -1,\n",
    "        'overwrite_cache': True,\n",
    "        'data_dir': output_dir,\n",
    "        'model_name_or_path':'microsoft/layoutlm-base-uncased',\n",
    "        'max_seq_length': 512,\n",
    "        'model_type': 'layoutlm',}\n",
    "\n",
    "# class to turn the keys of a dict into attributes (thanks Stackoverflow)\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "args = AttrDict(args)\n",
    "\n",
    "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
    "\n",
    "# the LayoutLM authors already defined a specific FunsdDataset, so we are going to use this here\n",
    "train_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=\"train\")\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=1)\n",
    "\n",
    "eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=\"test\")\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset,\n",
    "                             sampler=eval_sampler,\n",
    "                            batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "RhINSBw9I24G",
    "outputId": "914b2d2e-3fe1-40c8-ae96-f9a63a7cfa31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] aso electrical trading sdn bhd 1000131 - k no 31g, jalan sepadu c 25 / c, section 25, taman industries, axis 40400 shah alam, selangor. tel : 03 - 51221701, 51313091 fax : 03 - 51215716 gst no : 000683900928 tax invoice bill to : receipt # : cs00087400 date : 27 / 09 / 2017 salesperson : cashier : user time : 10 : 51 : 00 ( gst ) ( gst ) item qty rsp rsp amount 107636 3 78. 00 82. 68 248. 04 sr : hager timer, 24hrs power reserve tot qty : 3 248. 04 ( excluded gst ) sub total : 234. 00 discount : 0. 00 total gst : 14. 04 rounding : 0. 01 total : 248. 05 cash : 248. 015 change : 0. 00 gst summary tax code % amount gst sr 6 234. 00 14. 04 total : 234. 00 14. 04 goods sold are not returnable, thank you. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "input_ids = batch[0][0]\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66cEmLDoUFcm"
   },
   "source": [
    "## Define and fine-tune the model\n",
    "\n",
    "As this is a sequence labeling task, we are going to load `LayoutLMForTokenClassification` (the base sized model) from the hub. We are going to fine-tune it on a downstream task, namely FUNSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIdOsFBiTsuw",
    "outputId": "47f1e6e6-9fd0-43de-bd5b-7ab59ad78019"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlm-base-uncased were not used when initializing LayoutLMForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LayoutLMForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayoutLMForTokenClassification(\n",
       "  (layoutlm): LayoutLMModel(\n",
       "    (embeddings): LayoutLMEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (x_position_embeddings): Embedding(1024, 768)\n",
       "      (y_position_embeddings): Embedding(1024, 768)\n",
       "      (h_position_embeddings): Embedding(1024, 768)\n",
       "      (w_position_embeddings): Embedding(1024, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LayoutLMEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LayoutLMLayer(\n",
       "          (attention): LayoutLMAttention(\n",
       "            (self): LayoutLMSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LayoutLMOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LayoutLMPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LayoutLMForTokenClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yu0qePs2cRKo",
    "outputId": "a0c74cc3-ecd5-4826-8ad8-b77913e32f2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/624 [00:00<01:41,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0, 206, 206, 206, 206,  86,  86,  86,  86,  73,  73,  80,  80, 131,\n",
      "         131, 131, 131,  49,  49,  63,  37,  37,  91,  91, 137, 137, 137,  88,\n",
      "          88,  88,  88,  91,  91,  88, 111, 111, 167, 167,  64,  64,  58,  58,\n",
      "         157, 157, 157, 157, 134, 134, 134, 134, 134, 157, 157, 157, 157, 143,\n",
      "         143, 143, 143, 235, 235, 235, 235, 235, 235,  59, 143, 143, 143,  26,\n",
      "         202,  88,  75,  66,  66,  38,  41,  41, 169, 169, 169, 169, 169,  44,\n",
      "          98,  98, 195, 195, 195, 195, 195, 153, 153, 153, 153, 104, 127,  68,\n",
      "          68,  65,  65, 114, 119, 119,  64,  64,  18,  21,  41,  41,  87,  41,\n",
      "         114, 114, 114, 114, 114,  91,  91, 115, 115,  95,  95, 153, 103, 103,\n",
      "         103, 103, 145, 145, 145, 131, 131, 131, 131,  68,  68, 180, 180, 180,\n",
      "         180, 180,  94,  94,  94, 209,  50,  50,  50,  58,  60,  60,  20,  79,\n",
      "         138, 138, 138, 138, 138,  78,  78,  78,  78,  99,  76,  76,  76,  14,\n",
      "          14,  78,  78,  78, 168, 168, 168,  73,  73,  73,  18,  17,  74,  74,\n",
      "          74,  51,  51,  51,  97,  97,  76,  76,  76,  13,  15,  94,  94,  94,\n",
      "          96,  50,  50, 177, 177, 101, 101, 101, 302, 302, 302, 302, 302, 302,\n",
      "          81,  81,  81,  16,  78,  78,  78, 169, 169, 169, 169, 169,  81,  81,\n",
      "          81,  14,  36,  90,  90,  90,  36,  37,  37, 113,  79,  79,  79,  39,\n",
      "          15,  39,  92,  92,  92,  32, 191, 191, 191, 191, 191,  76,  76,  76,\n",
      "          36,  14,  29,  89,  89,  89, 190, 190, 190, 190,  80,  80,  80,  18,\n",
      "          16,  87,  87,  87,  87, 112, 112,  62,  62,  62,  73,  73,  73,  18,\n",
      "          76,  76,  76,  97,  44,  44, 162, 123, 123, 123, 123, 102, 102, 102,\n",
      "         102, 100, 100, 100,  81,  81,  81,  70,  70,  70,  99,  49,  49, 165,\n",
      "         106, 106, 106, 107, 107, 107, 107,  96,  96,  96,  98,  49,  49, 182,\n",
      "         182,  95,  95,  95,  90, 165, 165, 165, 103, 103, 103,  25,  25, 117,\n",
      "         117, 117, 117, 117, 117, 151,  99, 101,  36,  78,  94, 187, 187,  55,\n",
      "         176,  43, 193, 193, 193, 193, 156,  37, 154,  39,  83, 177, 211,  37,\n",
      "          56, 111, 111,  62, 138, 138,  40,  57, 102,  44,  57, 204,  98,  38,\n",
      "          76,  57, 135, 135, 135,  18, 195,  81, 259, 259, 259, 108,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n",
      "Loss after 0 steps: 2.3617076873779297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/624 [00:00<03:19,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0, 128, 124,  35, 196,  34,  34, 185, 185, 185,  61,  61,  63,  63,\n",
      "         194, 194, 194, 194, 194, 194, 194, 165, 165, 165, 165,  64,  64,  66,\n",
      "          46, 248, 248, 248, 248, 248, 248,  78,  71,  71,  77,  77,  19,  97,\n",
      "          97, 137, 137, 114, 114,  94,  94, 145, 145, 145, 145,  48,  16, 203,\n",
      "         203, 203, 203, 203, 203,  53, 119, 119, 119,  20, 353, 353, 353, 353,\n",
      "         353, 353, 353, 353, 353, 353, 353, 146, 146, 146, 146, 146, 120, 120,\n",
      "          17, 142, 142, 142, 142, 103, 103, 103, 148, 148, 148, 148, 148,  61,\n",
      "          61, 114,  85,  77,  77,  59,  59, 264, 264, 264, 264, 264, 264, 264,\n",
      "         264, 134, 134, 134, 134, 134,  94,  94,  94,  19, 103, 100, 100, 130,\n",
      "         130, 130, 130, 177, 177, 264, 264, 264, 264, 264, 264, 264, 128, 128,\n",
      "         128, 128, 128,  92,  92,  92,  20, 101,  97,  97, 127, 127, 127, 127,\n",
      "         142, 265, 265, 265, 265, 265, 265, 265, 265, 265, 128, 128, 128, 128,\n",
      "         128,  89,  89,  89,  21,  98,  98,  19,  82,  69,  71,  71,  21,  57,\n",
      "          57,  90,  90,  90,  82,  82,  82,  21, 172,  70,  70,  70, 101,  87,\n",
      "          87,  87, 122,  64,  84,  84,  82,  82, 123, 123, 123, 123,  89,  89,\n",
      "          89, 132,  71,  71,  71,  66,  66, 139,  99,  55, 133,  17,  18,  54,\n",
      "          54,  85,  85,  85,  69,  69,  69,  19,  20,  57,  57,  73,  73,  73,\n",
      "          72,  72,  72,  95,  60,  16,  63,  56,  92,  30,  30, 100, 106, 122,\n",
      "          37,  79,  42, 106, 156, 100,  21,  82,  42, 146, 110, 221, 221, 221,\n",
      "         221, 122,  21, 136, 120, 120,  47, 115, 115,  34, 136, 136,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.92 GiB total capacity; 1.97 GiB already allocated; 33.62 MiB free; 1.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3544305/4139162272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n\u001b[0m\u001b[1;32m     24\u001b[0m                       labels=labels)\n\u001b[1;32m     25\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/layoutlm/modeling_layoutlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1169\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m         outputs = self.layoutlm(\n\u001b[0m\u001b[1;32m   1172\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/layoutlm/modeling_layoutlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )\n\u001b[0;32m--> 826\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    827\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/layoutlm/modeling_layoutlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 )\n\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    498\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/layoutlm/modeling_layoutlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/layoutlm/modeling_layoutlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     ):\n\u001b[0;32m--> 310\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/layoutlm/modeling_layoutlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Mask heads if we want to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.92 GiB total capacity; 1.97 GiB already allocated; 33.62 MiB free; 1.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "global_step = 0\n",
    "num_train_epochs = 10\n",
    "t_total = len(train_dataloader) * num_train_epochs # total number of training steps \n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=num_train_epochs)\n",
    "\n",
    "#put the model in training mode\n",
    "model.train()\n",
    "for epoch in range(num_train_epochs):\n",
    "  for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "      input_ids = batch[0].to(device)\n",
    "      bbox = batch[4].to(device)\n",
    "      print(bbox[:, :, 2] - bbox[:, :, 0])\n",
    "      attention_mask = batch[1].to(device)\n",
    "      token_type_ids = batch[2].to(device)\n",
    "      labels = batch[3].to(device)\n",
    "\n",
    "      # forward pass\n",
    "      outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                      labels=labels)\n",
    "      loss = outputs.loss\n",
    "\n",
    "      # print loss every 100 steps\n",
    "      if global_step % 10 == 0:\n",
    "        print(f\"Loss after {global_step} steps: {loss.item()}\")\n",
    "\n",
    "      # backward pass to get the gradients \n",
    "      loss.backward()\n",
    "\n",
    "      #print(\"Gradients on classification head:\")\n",
    "      #print(model.classifier.weight.grad[6,:].sum())\n",
    "\n",
    "      # update\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNsh67yn5XFb"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now let's evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u1rNslap5Y3N",
    "outputId": "8bf958af-156d-49cc-a079-3a93ef6931cb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import (\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "eval_loss = 0.0\n",
    "nb_eval_steps = 0\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "\n",
    "# put model in evaluation mode\n",
    "model.eval()\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch[0].to(device)\n",
    "        bbox = batch[4].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        token_type_ids = batch[2].to(device)\n",
    "        labels = batch[3].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                        labels=labels)\n",
    "        # get the loss and logits\n",
    "        tmp_eval_loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        # compute the predictions\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = labels.detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(\n",
    "                out_label_ids, labels.detach().cpu().numpy(), axis=0\n",
    "            )\n",
    "\n",
    "# compute average evaluation loss\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "preds = np.argmax(preds, axis=2)\n",
    "\n",
    "out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "for i in range(out_label_ids.shape[0]):\n",
    "    for j in range(out_label_ids.shape[1]):\n",
    "        if out_label_ids[i, j] != pad_token_label_id:\n",
    "            out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "            preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "results = {\n",
    "    \"loss\": eval_loss,\n",
    "    \"precision\": precision_score(out_label_list, preds_list),\n",
    "    \"recall\": recall_score(out_label_list, preds_list),\n",
    "    \"f1\": f1_score(out_label_list, preds_list),\n",
    "}\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzOwxxIHM30r"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now comes the fun part! We can now use the fine-tuned model and test it on unseen data.\n",
    "\n",
    "Note that LayoutLM relies on an external OCR engine (it's not end-to-end -> that's probably something for the future). The test data itself also contains the annotated bounding boxes, but let's run an OCR engine ourselves.\n",
    "\n",
    "So let's load in a image of the test set, run our own OCR on it to get the bounding boxes, then run LayoutLM on the individual tokens and visualize the result!\n",
    "\n",
    "Sources:\n",
    "* https://www.kaggle.com/jpmiller/layoutlm-starter\n",
    "* https://bhadreshpsavani.medium.com/how-to-use-tesseract-library-for-ocr-in-google-colab-notebook-5da5470e4fe0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset and Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! wget https://t3638486.p.clickup-attachments.com/t3638486/ab080a87-3910-47e2-96e2-7143ad53ab82/test_data.zip\n",
    "! unzip test_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMForTokenClassification, LayoutLMTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = LayoutLMTokenizer.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.jit.load(\"traced_sroie.pt\")\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def convert_b64(input_file_name):\n",
    "    \"\"\"Open image and convert it to Base64\"\"\"\n",
    "    with open(input_file_name, \"rb\") as input_file:\n",
    "        jpeg_bytes = base64.b64encode(input_file.read()).decode(\"utf-8\")\n",
    "    return jpeg_bytes\n",
    "\n",
    "image_file_name = '/home/fsmlp/Downloads/test_data/X51009453729.jpg'\n",
    "instance = {\"data\": {\"b64\": convert_b64(image_file_name)}}\n",
    "\n",
    "# res = requests.post(\"http://localhost:7080/wfpredict/ocr\", json={\"instances\": [instance]})\n",
    "res = requests.post(\"http://164.52.218.27:7080/wfpredict/ocr\", json={\"instances\": [instance]})\n",
    "dictFromServer = res.json()\n",
    "\n",
    "df = pd.DataFrame(dictFromServer[\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(image_file_name)\n",
    "image = image.convert(\"RGB\")\n",
    "width, height = image.size\n",
    "\n",
    "bboxes = df['bbox'].tolist()\n",
    "def format_box(box, width, height):\n",
    "    return ([\n",
    "        int(1000 * box[0]),\n",
    "        int(1000 * box[1]),\n",
    "        int(1000 * box[2]),\n",
    "        int(1000 * box[3]),\n",
    "    ], [\n",
    "        int(box[0] * width),\n",
    "        int(box[1] * height),\n",
    "        int(box[2] * width),\n",
    "        int(box[3] * height),\n",
    "    ])\n",
    "  \n",
    "actual_boxes = []\n",
    "boxes = []\n",
    "for box in bboxes:\n",
    "    a,b=format_box(box, width, height)\n",
    "    boxes.append(a)\n",
    "    actual_boxes.append(b)\n",
    "\n",
    "words = df['ocr'].tolist()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDEh152kB-LE"
   },
   "outputs": [],
   "source": [
    "def convert_example_to_features(image, words, boxes, actual_boxes, tokenizer, max_seq_length=512, cls_token_box=[0, 0, 0, 0],\n",
    "                                 sep_token_box=[1000, 1000, 1000, 1000],\n",
    "                                 pad_token_box=[0, 0, 0, 0]):\n",
    "      width, height = image.size\n",
    "\n",
    "      tokens = []\n",
    "      token_boxes = []\n",
    "      actual_bboxes = [] # we use an extra b because actual_boxes is already used\n",
    "      token_actual_boxes = []\n",
    "      ocr_boxes = []\n",
    "      for word, box, actual_bbox in zip(words, boxes, actual_boxes):\n",
    "          word_tokens = tokenizer.tokenize(word)\n",
    "          tokens.extend(word_tokens)\n",
    "          token_boxes.extend([box] * len(word_tokens))\n",
    "          actual_bboxes.extend([actual_bbox] * len(word_tokens))\n",
    "          token_actual_boxes.extend([actual_bbox] * len(word_tokens))\n",
    "          ocr_boxes.extend([word]* len(word_tokens))\n",
    "\n",
    "      # Truncation: account for [CLS] and [SEP] with \"- 2\". \n",
    "      special_tokens_count = 2 \n",
    "      if len(tokens) > max_seq_length - special_tokens_count:\n",
    "          tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "          token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n",
    "          actual_bboxes = actual_bboxes[: (max_seq_length - special_tokens_count)]\n",
    "          token_actual_boxes = token_actual_boxes[: (max_seq_length - special_tokens_count)]\n",
    "          ocr_boxes = ocr_boxes[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "      # add [SEP] token, with corresponding token boxes and actual boxes\n",
    "      tokens += [tokenizer.sep_token]\n",
    "      token_boxes += [sep_token_box]\n",
    "      actual_bboxes += [[0, 0, width, height]]\n",
    "      token_actual_boxes += [[0, 0, width, height]]\n",
    "      ocr_boxes += ['']\n",
    "      \n",
    "      segment_ids = [0] * len(tokens)\n",
    "\n",
    "      # next: [CLS] token\n",
    "      tokens = [tokenizer.cls_token] + tokens\n",
    "      token_boxes = [cls_token_box] + token_boxes\n",
    "      actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n",
    "      token_actual_boxes = [[0, 0, width, height]] + token_actual_boxes\n",
    "      ocr_boxes = [''] + ocr_boxes\n",
    "      segment_ids = [1] + segment_ids\n",
    "\n",
    "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "      # tokens are attended to.\n",
    "      input_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      padding_length = max_seq_length - len(input_ids)\n",
    "      input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "      input_mask += [0] * padding_length\n",
    "      segment_ids += [tokenizer.pad_token_id] * padding_length\n",
    "      token_boxes += [pad_token_box] * padding_length\n",
    "      token_actual_boxes += [pad_token_box] * padding_length\n",
    "      ocr_boxes += [pad_token_box] * padding_length\n",
    "\n",
    "      assert len(input_ids) == max_seq_length\n",
    "      assert len(input_mask) == max_seq_length\n",
    "      assert len(segment_ids) == max_seq_length\n",
    "      assert len(token_boxes) == max_seq_length\n",
    "      assert len(token_actual_boxes) == max_seq_length\n",
    "      assert len(ocr_boxes) == max_seq_length\n",
    "      \n",
    "      return input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes, ocr_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Omze6dqfUTH-"
   },
   "outputs": [],
   "source": [
    "input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes, ocr_boxes = convert_example_to_features(image=image, words=words, boxes=boxes, actual_boxes=actual_boxes, tokenizer=tokenizer)\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QrrjzMA6Xps",
    "outputId": "5b15e45d-19b3-4c26-e7d9-1acd11687ad9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "attention_mask = torch.tensor(input_mask, device=device).unsqueeze(0)\n",
    "token_type_ids = torch.tensor(segment_ids, device=device).unsqueeze(0)\n",
    "bbox = torch.tensor(token_boxes, device=device).unsqueeze(0)\n",
    "# traced_model = torch.jit.trace(model, [input_ids,bbox,attention_mask,token_type_ids])\n",
    "# torch.jit.save(traced_model,'traced_sroie.pt')\n",
    "outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_predictions = outputs[0].argmax(-1).squeeze().tolist() # the predictions are at the token level\n",
    "# token_predictions = outputs.logits.argmax(-1).squeeze().tolist() # the predictions are at the token level\n",
    "\n",
    "word_level_predictions = [] # let's turn them into word level predictions\n",
    "t_boxes = []\n",
    "final_boxes = []\n",
    "ocr_results = []\n",
    "for id, token_pred, box, tb, ocr in zip(input_ids.squeeze().tolist(), token_predictions, token_actual_boxes, token_boxes, ocr_boxes):\n",
    "  if (tokenizer.decode([id]).startswith(\"##\")) or (id in [tokenizer.cls_token_id, \n",
    "                                                           tokenizer.sep_token_id, \n",
    "                                                          tokenizer.pad_token_id]):\n",
    "    # skip prediction + bounding box\n",
    "\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(token_pred)\n",
    "    final_boxes.append(box)\n",
    "    t_boxes.append(tb)\n",
    "    ocr_results.append(ocr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg7C1_PReCXU"
   },
   "source": [
    "Compare this to the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "label2color = {'company':'blue', 'date':'green', 'address':'orange',\"total\":'violet','#other':'grey'}\n",
    "label_map = dict(zip(range(5), label2color.keys()))\n",
    "print(label_map)\n",
    "predictions = []\n",
    "for prediction, box, tb, ocr in zip(word_level_predictions, final_boxes, t_boxes, ocr_results):\n",
    "    predicted_label = label_map[prediction]\n",
    "    rel_box = [b/1000 for b in tb]\n",
    "    json_result = {}\n",
    "    json_result[\"bbox\"] = rel_box\n",
    "    json_result[\"ocr\"] = ocr\n",
    "    json_result[\"key\"] = predicted_label\n",
    "    predictions.append(json_result)\n",
    "    \n",
    "    if predicted_label!='other':\n",
    "        # print(predicted_label, box)\n",
    "        draw.rectangle(box, outline=label2color[predicted_label])\n",
    "        draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill=label2color[predicted_label], font=font)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Tesseract or upload any other OCR results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgrZ8pENAZHD",
    "outputId": "1ea8100f-f4f0-490f-dd91-456ca8cdf6d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo apt install tesseract-ocr\n",
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l_7dmPC64Dfd",
    "outputId": "078afc7c-27a2-4614-f75a-a1c7f9870b9a"
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "image = Image.open(\"/home/fsmlp/Downloads/test_data/X51009453729.jpg\")\n",
    "image = image.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "width, height = image.size\n",
    "w_scale = 1000/width\n",
    "h_scale = 1000/height\n",
    "\n",
    "ocr_df = pytesseract.image_to_data(image, output_type='data.frame') \\\n",
    "            \n",
    "ocr_df = ocr_df.dropna() \\\n",
    "               .assign(left_scaled = ocr_df.left*w_scale,\n",
    "                       width_scaled = ocr_df.width*w_scale,\n",
    "                       top_scaled = ocr_df.top*h_scale,\n",
    "                       height_scaled = ocr_df.height*h_scale,\n",
    "                       right_scaled = lambda x: x.left_scaled + x.width_scaled,\n",
    "                       bottom_scaled = lambda x: x.top_scaled + x.height_scaled)\n",
    "\n",
    "float_cols = ocr_df.select_dtypes('float').columns\n",
    "ocr_df[float_cols] = ocr_df[float_cols].round(0).astype(int)\n",
    "ocr_df = ocr_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "ocr_df = ocr_df.dropna().reset_index(drop=True)\n",
    "ocr_df[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgtCwrvK3k2W"
   },
   "source": [
    "Here we create a list of words, actual bounding boxes, and normalized boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qoB1uEtOP_2H",
    "outputId": "9f4e04a0-4e7d-496c-f731-4a60af5cadc5"
   },
   "outputs": [],
   "source": [
    "words = list(ocr_df.text)\n",
    "coordinates = ocr_df[['left', 'top', 'width', 'height']]\n",
    "actual_boxes = []\n",
    "for idx, row in coordinates.iterrows():\n",
    "  x, y, w, h = tuple(row) # the row comes in (left, top, width, height) format\n",
    "  actual_box = [x, y, x+w, y+h] # we turn it into (left, top, left+widght, top+height) to get the actual box \n",
    "  actual_boxes.append(actual_box)\n",
    "\n",
    "def normalize_box(box, width, height):\n",
    "    return [\n",
    "        int(1000 * (box[0] / width)),\n",
    "        int(1000 * (box[1] / height)),\n",
    "        int(1000 * (box[2] / width)),\n",
    "        int(1000 * (box[3] / height)),\n",
    "    ]\n",
    "\n",
    "boxes = []\n",
    "for box in actual_boxes:\n",
    "  boxes.append(normalize_box(box, width, height))\n",
    "# boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfneTqU875U1"
   },
   "source": [
    "This should become the future API of LayoutLMTokenizer (`prepare_for_model()`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDEh152kB-LE"
   },
   "outputs": [],
   "source": [
    "def convert_example_to_features(image, words, boxes, actual_boxes, tokenizer, args, cls_token_box=[0, 0, 0, 0],\n",
    "                                 sep_token_box=[1000, 1000, 1000, 1000],\n",
    "                                 pad_token_box=[0, 0, 0, 0]):\n",
    "      width, height = image.size\n",
    "\n",
    "      tokens = []\n",
    "      token_boxes = []\n",
    "      actual_bboxes = [] # we use an extra b because actual_boxes is already used\n",
    "      token_actual_boxes = []\n",
    "      for word, box, actual_bbox in zip(words, boxes, actual_boxes):\n",
    "          word_tokens = tokenizer.tokenize(word)\n",
    "          tokens.extend(word_tokens)\n",
    "          token_boxes.extend([box] * len(word_tokens))\n",
    "          actual_bboxes.extend([actual_bbox] * len(word_tokens))\n",
    "          token_actual_boxes.extend([actual_bbox] * len(word_tokens))\n",
    "\n",
    "      # Truncation: account for [CLS] and [SEP] with \"- 2\". \n",
    "      special_tokens_count = 2 \n",
    "      if len(tokens) > args.max_seq_length - special_tokens_count:\n",
    "          tokens = tokens[: (args.max_seq_length - special_tokens_count)]\n",
    "          token_boxes = token_boxes[: (args.max_seq_length - special_tokens_count)]\n",
    "          actual_bboxes = actual_bboxes[: (args.max_seq_length - special_tokens_count)]\n",
    "          token_actual_boxes = token_actual_boxes[: (args.max_seq_length - special_tokens_count)]\n",
    "\n",
    "      # add [SEP] token, with corresponding token boxes and actual boxes\n",
    "      tokens += [tokenizer.sep_token]\n",
    "      token_boxes += [sep_token_box]\n",
    "      actual_bboxes += [[0, 0, width, height]]\n",
    "      token_actual_boxes += [[0, 0, width, height]]\n",
    "      \n",
    "      segment_ids = [0] * len(tokens)\n",
    "\n",
    "      # next: [CLS] token\n",
    "      tokens = [tokenizer.cls_token] + tokens\n",
    "      token_boxes = [cls_token_box] + token_boxes\n",
    "      actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n",
    "      token_actual_boxes = [[0, 0, width, height]] + token_actual_boxes\n",
    "      segment_ids = [1] + segment_ids\n",
    "\n",
    "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "      # tokens are attended to.\n",
    "      input_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      padding_length = args.max_seq_length - len(input_ids)\n",
    "      input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "      input_mask += [0] * padding_length\n",
    "      segment_ids += [tokenizer.pad_token_id] * padding_length\n",
    "      token_boxes += [pad_token_box] * padding_length\n",
    "      token_actual_boxes += [pad_token_box] * padding_length\n",
    "\n",
    "      assert len(input_ids) == args.max_seq_length\n",
    "      assert len(input_mask) == args.max_seq_length\n",
    "      assert len(segment_ids) == args.max_seq_length\n",
    "      #assert len(label_ids) == args.max_seq_length\n",
    "      assert len(token_boxes) == args.max_seq_length\n",
    "      assert len(token_actual_boxes) == args.max_seq_length\n",
    "      \n",
    "      return input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Omze6dqfUTH-"
   },
   "outputs": [],
   "source": [
    "input_ids, input_mask, segment_ids, token_boxes, token_actual_boxes = convert_example_to_features(image=image, words=words, boxes=boxes, actual_boxes=actual_boxes, tokenizer=tokenizer, args=args)\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QrrjzMA6Xps",
    "outputId": "5b15e45d-19b3-4c26-e7d9-1acd11687ad9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "attention_mask = torch.tensor(input_mask, device=device).unsqueeze(0)\n",
    "token_type_ids = torch.tensor(segment_ids, device=device).unsqueeze(0)\n",
    "bbox = torch.tensor(token_boxes, device=device).unsqueeze(0)\n",
    "# traced_model = torch.jit.trace(model, [input_ids,bbox,attention_mask,token_type_ids])\n",
    "# torch.jit.save(traced_model,'traced_sroie.pt')\n",
    "outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_predictions = outputs[0].argmax(-1).squeeze().tolist() # the predictions are at the token level\n",
    "# token_predictions = outputs.logits.argmax(-1).squeeze().tolist() # the predictions are at the token level\n",
    "\n",
    "word_level_predictions = [] # let's turn them into word level predictions\n",
    "final_boxes = []\n",
    "for id, token_pred, box in zip(input_ids.squeeze().tolist(), token_predictions, token_actual_boxes):\n",
    "  if (tokenizer.decode([id]).startswith(\"##\")) or (id in [tokenizer.cls_token_id, \n",
    "                                                           tokenizer.sep_token_id, \n",
    "                                                          tokenizer.pad_token_id]):\n",
    "    # skip prediction + bounding box\n",
    "\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(token_pred)\n",
    "    final_boxes.append(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg7C1_PReCXU"
   },
   "source": [
    "Compare this to the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "draw = ImageDraw.Draw(image)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "def iob_to_label(label):\n",
    "  if label != 'O':\n",
    "    return label[2:]\n",
    "  else:\n",
    "    return \"other\"\n",
    "\n",
    "label2color = {'company':'blue', 'address':'green', 'total':'orange', 'other':'grey', \"date\":'violet'}\n",
    "\n",
    "for prediction, box in zip(word_level_predictions, final_boxes):\n",
    "    predicted_label = iob_to_label(label_map[prediction]).lower()\n",
    "    if predicted_label!='other2':\n",
    "        # print(predicted_label, box)\n",
    "        draw.rectangle(box, outline=label2color[predicted_label])\n",
    "        draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill=label2color[predicted_label], font=font)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b56fa3f096ed1baaf6cf839b7b0573d71bceaca1138b325c74ce04727c10ac7c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('torch-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
